{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Neural Network (One Hidden Layer) with Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> <b> <u> Dataset background:</u></b> </h2>\n",
    "<ul>\n",
    "    <li>Data: Diabetic Encounters (1-14 days/each) from 130 Hospitals for 10 years (1999-2008) </li>\n",
    "    <li>Goal: Predict if a diabetic patient will be readmitted to a hospital (less than 30 days, after 30 days, or never)</li>\n",
    "    <li>Target Feature: readmitted </li>\n",
    "    <li> <a href = \"https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\">Dataset Source</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#display all columns of dataframe\n",
    "pd.pandas.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset Shape: (101766, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>payer_code</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>diag_1</th>\n",
       "      <th>diag_2</th>\n",
       "      <th>diag_3</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>max_glu_serum</th>\n",
       "      <th>A1Cresult</th>\n",
       "      <th>metformin</th>\n",
       "      <th>repaglinide</th>\n",
       "      <th>nateglinide</th>\n",
       "      <th>chlorpropamide</th>\n",
       "      <th>glimepiride</th>\n",
       "      <th>acetohexamide</th>\n",
       "      <th>glipizide</th>\n",
       "      <th>glyburide</th>\n",
       "      <th>tolbutamide</th>\n",
       "      <th>pioglitazone</th>\n",
       "      <th>rosiglitazone</th>\n",
       "      <th>acarbose</th>\n",
       "      <th>miglitol</th>\n",
       "      <th>troglitazone</th>\n",
       "      <th>tolazamide</th>\n",
       "      <th>examide</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80218</td>\n",
       "      <td>247042512</td>\n",
       "      <td>88533495</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>BC</td>\n",
       "      <td>Emergency/Trauma</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>428</td>\n",
       "      <td>518</td>\n",
       "      <td>250.42</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101382</td>\n",
       "      <td>438525890</td>\n",
       "      <td>40711671</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>MC</td>\n",
       "      <td>?</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>414</td>\n",
       "      <td>786</td>\n",
       "      <td>425</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56463</td>\n",
       "      <td>162110298</td>\n",
       "      <td>86376672</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>MC</td>\n",
       "      <td>InternalMedicine</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "      <td>428</td>\n",
       "      <td>427</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68576</td>\n",
       "      <td>193669110</td>\n",
       "      <td>43385490</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>MC</td>\n",
       "      <td>?</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>285</td>\n",
       "      <td>250</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Down</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12375</td>\n",
       "      <td>50407170</td>\n",
       "      <td>1294047</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>Orthopedics-Reconstructive</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>722</td>\n",
       "      <td>250</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        encounter_id  patient_nbr             race  gender      age weight  \\\n",
       "80218      247042512     88533495        Caucasian    Male  [40-50)      ?   \n",
       "101382     438525890     40711671        Caucasian    Male  [70-80)      ?   \n",
       "56463      162110298     86376672        Caucasian    Male  [70-80)      ?   \n",
       "68576      193669110     43385490        Caucasian  Female  [70-80)      ?   \n",
       "12375       50407170      1294047  AfricanAmerican    Male  [40-50)      ?   \n",
       "\n",
       "        admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "80218                   2                         6                    7   \n",
       "101382                  1                         1                    7   \n",
       "56463                   2                         1                    7   \n",
       "68576                   1                         6                    7   \n",
       "12375                   3                         1                    1   \n",
       "\n",
       "        time_in_hospital payer_code           medical_specialty  \\\n",
       "80218                  5         BC            Emergency/Trauma   \n",
       "101382                 1         MC                           ?   \n",
       "56463                 12         MC            InternalMedicine   \n",
       "68576                  2         MC                           ?   \n",
       "12375                  1          ?  Orthopedics-Reconstructive   \n",
       "\n",
       "        num_lab_procedures  num_procedures  num_medications  \\\n",
       "80218                   68               6               38   \n",
       "101382                  44               0               17   \n",
       "56463                   86               1               31   \n",
       "68576                   52               2                9   \n",
       "12375                   51               1               10   \n",
       "\n",
       "        number_outpatient  number_emergency  number_inpatient diag_1 diag_2  \\\n",
       "80218                   2                 0                 8    428    518   \n",
       "101382                  0                 0                 1    414    786   \n",
       "56463                   0                 0                 0    486    428   \n",
       "68576                   0                 0                 0    569    285   \n",
       "12375                   0                 0                 0    722    250   \n",
       "\n",
       "        diag_3  number_diagnoses max_glu_serum A1Cresult metformin  \\\n",
       "80218   250.42                 9          None      None        No   \n",
       "101382     425                 9          None      None        Up   \n",
       "56463      427                 9          None      None    Steady   \n",
       "68576      250                 6          None      None        No   \n",
       "12375        ?                 2          None      None    Steady   \n",
       "\n",
       "       repaglinide nateglinide chlorpropamide glimepiride acetohexamide  \\\n",
       "80218           No          No             No          No            No   \n",
       "101382          No          No             No          No            No   \n",
       "56463           No          No             No          No            No   \n",
       "68576           No          No             No          No            No   \n",
       "12375           No          No             No          No            No   \n",
       "\n",
       "       glipizide glyburide tolbutamide pioglitazone rosiglitazone acarbose  \\\n",
       "80218         No        No          No           No            No       No   \n",
       "101382        No    Steady          No           No            No       No   \n",
       "56463         No        No          No           No        Steady       No   \n",
       "68576     Steady        No          No           No            No       No   \n",
       "12375         No        No          No           No            No       No   \n",
       "\n",
       "       miglitol troglitazone tolazamide examide citoglipton insulin  \\\n",
       "80218        No           No         No      No          No      Up   \n",
       "101382       No           No         No      No          No      No   \n",
       "56463        No           No         No      No          No      No   \n",
       "68576        No           No         No      No          No    Down   \n",
       "12375        No           No         No      No          No  Steady   \n",
       "\n",
       "       glyburide-metformin glipizide-metformin glimepiride-pioglitazone  \\\n",
       "80218                   No                  No                       No   \n",
       "101382                  No                  No                       No   \n",
       "56463                   No                  No                       No   \n",
       "68576                   No                  No                       No   \n",
       "12375                   No                  No                       No   \n",
       "\n",
       "       metformin-rosiglitazone metformin-pioglitazone change diabetesMed  \\\n",
       "80218                       No                     No     Ch         Yes   \n",
       "101382                      No                     No     Ch         Yes   \n",
       "56463                       No                     No     Ch         Yes   \n",
       "68576                       No                     No     Ch         Yes   \n",
       "12375                       No                     No     Ch         Yes   \n",
       "\n",
       "       readmitted  \n",
       "80218         >30  \n",
       "101382         NO  \n",
       "56463          NO  \n",
       "68576         >30  \n",
       "12375          NO  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset \n",
    "dataset_url = \"https://raw.githubusercontent.com/ronakHegde98/CS-4372-Computational-Methods-for-Data-Scientists/master/data/diabetic_data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "print(f\"Initial Dataset Shape: {df.shape}\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16773 patients with multiple records\n"
     ]
    }
   ],
   "source": [
    "## check if patients have multiple records\n",
    "print(f\"There are {np.sum(df['patient_nbr'].value_counts() > 1)} patients with multiple records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37 categorical columns and 13 numerical columns\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [col for col in df.columns if df[col].dtype == np.dtype(np.object)]\n",
    "print(f\"There are {len(categorical_cols)} categorical columns and {len(df.columns)-len(categorical_cols)} numerical columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Handling Missing Values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 192849 '?' values in our dataset which is approx 3.79% of our entire dataset\n"
     ]
    }
   ],
   "source": [
    "## sum all missing values for each row of df (axis 0 is row)\n",
    "missing_count = np.sum(np.sum(np.equal(df, '?'), axis=0))\n",
    "print(f\"There are {missing_count} '?' values in our dataset which is approx {np.round((missing_count/(np.multiply(df.shape[0], df.shape[1])))*100,2)}% of our entire dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert ?'s into np.nan\n",
    "df.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing data\n",
      " race: 2273\n",
      " weight: 98569\n",
      " payer_code: 40256\n",
      " medical_specialty: 49949\n",
      " diag_1: 21\n",
      " diag_2: 358\n",
      " diag_3: 1423\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns with missing data\")\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "for col in missing_cols:\n",
    "    print(' ' + col + ': ' + str(df[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop rows where gender is Unknown/Invalid\n",
    "df.drop(df[df['gender'] == \"Unknown/Invalid\"].index, axis=0, inplace=True)\n",
    "\n",
    "## dropping columns that have many missing values\n",
    "dropped_columns = ['weight', 'payer_code', 'medical_specialty']\n",
    "dropped_columns.append(\"encounter_id\")\n",
    "dropped_columns.append('discharge_disposition_id')\n",
    "\n",
    "## dropping columns that have little to no variability\n",
    "for col in categorical_cols:\n",
    "    if(df[col].value_counts(normalize=True).max() > 0.948):\n",
    "        dropped_columns.append(col)\n",
    "        \n",
    "df.drop(columns=dropped_columns, axis=1, inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Some Patients have multiple records </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one record per patient (where they had max of time_in_hospital)\n",
    "df = df.loc[df.groupby(\"patient_nbr\", sort=False)['time_in_hospital'].idxmax()]\n",
    "df.drop(columns = ['patient_nbr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## convert our categorical variable (if readmitted -> 1 else 0)\n",
    "df['readmitted'] = np.where(df['readmitted']!='NO',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert age ranges to the midpoint of the ranges\n",
    "new_ages = {\n",
    "    \"[0-10)\": 5,\n",
    "    \"[10-20)\": 15,\n",
    "    \"[20-30)\": 25,\n",
    "    \"[30-40)\": 35,\n",
    "    \"[40-50)\": 45,\n",
    "    \"[50-60)\": 55,\n",
    "    \"[60-70)\": 65,\n",
    "    \"[70-80)\": 75,\n",
    "    \"[80-90)\": 85,\n",
    "    \"[90-100)\": 95\n",
    "}\n",
    "\n",
    "df['age'] = df['age'].map(new_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_glu_serums = {\n",
    "    \"None\": 0,\n",
    "    \"Norm\": 100,\n",
    "    \">200\": 200,\n",
    "    \">300\": 300\n",
    "}\n",
    "df['max_glu_serum'] = df['max_glu_serum'].map(max_glu_serums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1CResult_map = {\n",
    "    \"None\": 0,\n",
    "    \"Norm\": 5,\n",
    "    \">7\": 7,\n",
    "    \">8\": 8\n",
    "}\n",
    "df['A1Cresult'] = df['A1Cresult'].map(A1CResult_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting binary variables into -1 or 1\n",
    "df['change'] = np.where(df['change']=='No',-1,1)\n",
    "df['diabetesMed'] = np.where(df['diabetesMed']=='No',-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_codes = {\n",
    "    \"No\": -20,\n",
    "    \"Down\": -10, \n",
    "    \"Steady\": 0,\n",
    "    \"Up\": 10    \n",
    "}\n",
    "drugs = ['metformin','glipizide','glyburide', 'pioglitazone', 'rosiglitazone','insulin'] \n",
    "for drug in drugs:\n",
    "    df[drug] = df[drug].map(drug_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## mapping diagnosis categories according to paper (else 800 plus features)\n",
    "diagnosis_cols = ['diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "for col in diagnosis_cols:\n",
    "    df['tmp'] = np.nan\n",
    "    df.loc[(df[col].str.contains(\"250\")), col] = '250'\n",
    "    df.loc[(df[col].str.startswith('V')) | (df[col].str.startswith('E')), col] = '-999' \n",
    "\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "    #convert the correct ranges based on values given in paper\n",
    "    df.loc[(((df[col] >=390) & (df[col]<=460)) | (df[col] == 785)), 'tmp'] = 'Circulatory'\n",
    "    df.loc[(((df[col] >=460) & (df[col]<=519)) | (df[col] == 786)), 'tmp'] = 'Respiratory'\n",
    "    df.loc[(((df[col] >=520) & (df[col]<=579)) | (df[col] == 787)), 'tmp'] = 'Digestive'\n",
    "    df.loc[(((df[col] >=580) & (df[col]<=629)) | (df[col] == 788)), 'tmp'] = 'Genitourinary'\n",
    "    df.loc[((df[col] >=800) & (df[col]<=999)), 'tmp'] = 'Injury'\n",
    "    df.loc[((df[col] >=710) & (df[col]<=739)), 'tmp'] = 'Musculoskeletal'\n",
    "    df.loc[((df[col] >=140) & (df[col]<=239)), 'tmp'] = 'Neoplasms'\n",
    "    df.loc[(df[col] == 250), 'tmp'] = 'Diabetes'\n",
    "    \n",
    "    df['tmp'].fillna(value = \"Other\", inplace=True)\n",
    "    \n",
    "    df[col] = df['tmp']\n",
    "    df.drop(columns=['tmp'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## admission_source_id\n",
    "df['tmp'] = np.nan\n",
    "col = 'admission_source_id'\n",
    "df.loc[((df[col].between(4,6)) | (df[col] == 10) | (df[col] == 18) | (df[col] == 22) | (df[col].between(25,26))), 'tmp'] = \"Transfer_Source\"\n",
    "df.loc[df[col].between(1,3), 'tmp'] = \"Referral_Source\"\n",
    "df.loc[((df[col].between(11,14))| (df[col].between(23,24))), 'tmp'] = \"Birth_Source\"\n",
    "df.loc[df[col] == 7, 'tmp'] = \"Emergency_Source\"\n",
    "df.loc[((df[col] == 8) | (df[col]==19)), 'tmp'] = \"Other\"\n",
    "        \n",
    "df['tmp'].fillna(value = \"Unknown\", inplace=True)\n",
    "df[col] = df['tmp']\n",
    "df.drop(columns=['tmp'], inplace=True)\n",
    "\n",
    "\n",
    "##mapping admission type_id\n",
    "df['tmp'] = np.nan\n",
    "col = 'admission_type_id'\n",
    "df.loc[df[col] == 1, 'tmp'] = 'Emergency_Type'\n",
    "df.loc[df[col] == 2, 'tmp'] = 'Urgent_Type'\n",
    "df.loc[df[col] == 3, 'tmp'] = 'Elective_Type'\n",
    "df.loc[df[col] == 7, 'tmp'] = 'Trauma_Type'\n",
    "df.loc[df[col] == 4, 'tmp'] = 'Newborn_Type'\n",
    "\n",
    "df['tmp'].fillna(value = \"Unknown\", inplace=True)\n",
    "df[col] = df['tmp']\n",
    "df.drop(columns=['tmp'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, cols):\n",
    "    \"\"\"one-hot encoding function for all our categorical columns\"\"\"\n",
    "    \n",
    "    for col in cols:\n",
    "        if(\"admission\" in col):\n",
    "            dummies = pd.get_dummies(df[col], drop_first=False)\n",
    "        else:\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)   \n",
    "        df.drop([col],axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding \n",
    "categorical_columns = [col for col in df.columns if df[col].dtype == np.dtype(object)]\n",
    "df = one_hot_encoder(df, categorical_columns)\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "#train-test-split\n",
    "target_variable = 'readmitted'\n",
    "Y_feature = df[target_variable]\n",
    "X_features = df.drop(columns=[target_variable])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features,Y_feature, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize of numerical columns\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(mm_scaler.fit_transform(X_train), columns = X_train.columns) \n",
    "X_test = pd.DataFrame(mm_scaler.fit_transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.values.reshape(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class NeuralNet:\n",
    "\n",
    "    def __init__(self, X_train, y_train, h=4):\n",
    "        #np.random.seed(1)\n",
    "        # h represents the number of neurons in the hidden layers\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "\n",
    "        # Find number of input and output layers from the dataset\n",
    "        input_layer_size = self.X.shape[0]\n",
    "        \n",
    "        \n",
    "        self.output_layer_size = 1\n",
    "\n",
    "        # assign random weights to matrices in network\n",
    "        # number of weights connecting layers = (no. of nodes in previous layer) x (no. of nodes in following layer)\n",
    "        self.W_hidden = 2 * np.random.random((h, input_layer_size)) - 1\n",
    "        self.Wb_hidden = 2 * np.random.random((h,1)) - 1\n",
    "\n",
    "        self.W_output = 2 * np.random.random((self.output_layer_size,h)) - 1\n",
    "        self.Wb_output = np.ones((self.output_layer_size,1))\n",
    "\n",
    "        self.deltaOut = np.zeros((self.output_layer_size, 1))\n",
    "        self.deltaHidden = np.zeros((h, 1))\n",
    "        self.h = h\n",
    "            \n",
    "\n",
    "    def __activation(self, x, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            self.__sigmoid(self, x)\n",
    "        elif activation == \"tanh\":\n",
    "            self.__tanh(self,x)\n",
    "        elif activation == \"relu\":\n",
    "            self.__relu(self,x)\n",
    "     \n",
    "\n",
    "    def __activation_derivative(self, x, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            self.__sigmoid_derivative(self, x)\n",
    "        elif activation == \"tanh\":\n",
    "            self.__tanh_derivative(self,x)\n",
    "        elif activation == \"relu\":\n",
    "            self.__relu_derivative(self,x)\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __tanh_derivative(self, x):\n",
    "        return 1-(np.tanh(x))**2\n",
    "    \n",
    "    def __relu_derivative(self,x):\n",
    "        return (x>0)*1\n",
    "\n",
    "\n",
    "    # Below is the training function\n",
    "    def train(self, activation, max_iterations=100, learning_rate=0.00001, momentum = 0.90):\n",
    "        \n",
    "        update_weight_output, update_weight_output_b, update_weight_hidden, update_weight_hidden_b = 0,0,0,0\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            out = self.forward_pass(activation)\n",
    "            \n",
    "            error = 0.5 * np.power((out - self.y), 2)\n",
    "            \n",
    "            \n",
    "            self.past_delta = [deepcopy(update_weight_output),\n",
    "                                        deepcopy(update_weight_output_b),\n",
    "                                        deepcopy(update_weight_hidden),\n",
    "                                        deepcopy(update_weight_hidden_b)]\n",
    "            \n",
    "            \n",
    "            self.backward_pass(out, activation)\n",
    "            \n",
    "            update_weight_output = learning_rate * (1-momentum) * np.dot(self.deltaOut,self.X_hidden.T) + momentum*self.past_delta[0]\n",
    "            \n",
    "            update_weight_output_b = learning_rate * (1-momentum) * np.dot(self.deltaOut, np.ones((np.size(self.X, 1), 1))) + momentum*self.past_delta[1]\n",
    "            \n",
    "            update_weight_hidden = learning_rate * (1-momentum)* np.dot(self.deltaHidden,self.X.T) + momentum*self.past_delta[2]\n",
    "            \n",
    "            update_weight_hidden_b = learning_rate * (1-momentum)* np.dot(self.deltaHidden,np.ones((np.size(self.X, 1), 1))) + momentum*self.past_delta[3]\n",
    "\n",
    "            self.W_output += update_weight_output\n",
    "            self.Wb_output += update_weight_output_b\n",
    "            self.W_hidden += update_weight_hidden\n",
    "            self.Wb_hidden += update_weight_hidden_b\n",
    "            \n",
    "\n",
    "        print(\"After \" + str(max_iterations) + \" iterations, the total error is \" + str(np.average(np.sum(error))))\n",
    "#         print(\"The final weight vectors are (starting from input to output layers) \\n\" + str(self.W_hidden))\n",
    "#         print(\"The final weight vectors are (starting from input to output layers) \\n\" + str(self.W_output))\n",
    "\n",
    "#         print(\"The final bias vectors are (starting from input to output layers) \\n\" + str(self.Wb_hidden))\n",
    "#         print(\"The final bias vectors are (starting from input to output layers) \\n\" + str(self.Wb_output))\n",
    "\n",
    "    def forward_pass(self, activation):\n",
    "        # pass our inputs through our neural network\n",
    "        in_hidden = np.dot(self.W_hidden, self.X) + self.Wb_hidden\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.X_hidden = self.__sigmoid(in_hidden)\n",
    "        elif activation == \"tanh\":\n",
    "            self.X_hidden = self.__tanh(in_hidden)\n",
    "        elif activation == \"relu\":\n",
    "            self.X_hidden = self.__relu(in_hidden)\n",
    "\n",
    "        in_output = np.dot(self.W_output, self.X_hidden) + self.Wb_output\n",
    "        \n",
    "        # output \n",
    "        if activation == \"sigmoid\":\n",
    "            out = self.__sigmoid(in_output)\n",
    "        elif activation == \"tanh\":\n",
    "            out = self.__tanh(in_output)\n",
    "        elif activation == \"relu\":\n",
    "            out = self.__relu(in_output)\n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, out, activation):\n",
    "        # pass our inputs through our neural network\n",
    "        self.compute_output_delta(out, activation)\n",
    "        self.compute_hidden_delta(activation)\n",
    "        \n",
    "\n",
    "\n",
    "    def compute_output_delta(self, out, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            delta_output = (self.y - out) * (self.__sigmoid_derivative(out))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_output = (self.y - out) * (self.__tanh_derivative(out))\n",
    "        elif activation == \"relu\":\n",
    "            delta_output = (self.y - out) * (self.__relu_derivative(out))\n",
    "\n",
    "        self.deltaOut = delta_output\n",
    "\n",
    "    def compute_hidden_delta(self, activation):\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            delta_hidden_layer = (self.W_output.T.dot(self.deltaOut)) * (self.__sigmoid_derivative(self.X_hidden))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_hidden_layer = (self.W_output.T.dot(self.deltaOut)) * (self.__tanh_derivative(self.X_hidden))\n",
    "        elif activation == \"relu\":\n",
    "            delta_hidden_layer = (self.W_output.T.dot(self.deltaOut)) * (self.__relu_derivative(self.X_hidden))\n",
    "        \n",
    "        self.deltaHidden = delta_hidden_layer\n",
    "\n",
    "\n",
    "    def predict(self, X_test, y_test, activation):\n",
    "        print(\"inside predict\")\n",
    "        predict_hidden = np.dot(self.W_hidden, X_test) + self.Wb_hidden\n",
    "        \n",
    "        self.X_hidden = self.__relu(predict_hidden)\n",
    "        \n",
    "        if(activation == \"sigmoid\"):\n",
    "            self.X_hidden = self.__sigmoid(predict_hidden)\n",
    "        elif(activation==\"relu\"):\n",
    "            self.X_hidden = self.__relu(predict_hidden)\n",
    "        elif(activation == \"tanh\"):\n",
    "            self.X_hidden = self.__tanh(predict_hidden)\n",
    "        \n",
    "        predict_output = np.dot(self.W_output, self.X_hidden) + self.Wb_output\n",
    "        \n",
    "        if(activation == \"sigmoid\"):\n",
    "            out = self.__sigmoid(predict_output)\n",
    "        elif(activation==\"relu\"):\n",
    "            out = self.__relu(predict_output)\n",
    "        elif(activation == \"tanh\"):\n",
    "            out = self.__tanh(predict_output)\n",
    "        \n",
    "        \n",
    "        error = 0.5 * np.power((out - y_test), 2)\n",
    "        print(f\"Error on Test Dataset is {np.sum(error)}\")\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 iterations, the total error is 9505.5\n"
     ]
    }
   ],
   "source": [
    "nn_model = NeuralNet(X_train.iloc[:,0:9].T,y_train.T, h=20)\n",
    "nn_model.train(activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside predict\n",
      "Error on Test Dataset is 2343.5\n"
     ]
    }
   ],
   "source": [
    "predictions = nn_model.predict(X_test.iloc[:,0:9].T,y_test.T,activation=\"relu\")\n",
    "predictions = np.around(predictions, 0).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
